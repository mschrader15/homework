{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2\n",
    "\n",
    "Max Schrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding /home/max/Documents/Homework/homework/ME-691 to path\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVEMENTS = {\"A\": {\"moves\": [\"B\", \"C\", \"D\"], \"reward\": [1.0, 1.6, 1.9]},\n",
    "             \"B\": {\"moves\": [\"E\", \"F\", \"G\"], \"reward\": [1.0, 1.5, 1.0]},\n",
    "             \"C\": {\"moves\": [\"E\", \"F\", \"G\"], \"reward\": [1.8, 1.8, 1.7]},\n",
    "             \"D\": {\"moves\": [\"E\", \"F\", \"G\"], \"reward\": [1.9, 1.6, 1.5]},\n",
    "             \"E\": {\"moves\": [\"H\", \"L\", \"O\"], 'reward': [1.9, 1.6, 1.8]},\n",
    "             \"F\": {\"moves\": [\"H\", \"L\", \"O\"], 'reward': [1.4, 1.4, 1.8]},\n",
    "             \"G\": {\"moves\": [\"H\", \"L\", \"O\"], 'reward': [1.0, 1.1, 1.1]},\n",
    "             \"H\": {\"moves\": [\"P\"], 'reward': [1.3]},\n",
    "             \"L\": {\"moves\": [\"P\"], 'reward': [1.8]},\n",
    "             \"O\": {\"moves\": [\"P\"], 'reward': [1.8]},\n",
    "             \"P\": {\"moves\": [], 'reward': []}}\n",
    "NUM_STATES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_R(movements):\n",
    "    \"\"\"\n",
    "    compose_R creates the reward matrix from the dictionary above. It is just a helper function, \n",
    "    as the R matrix is hard to create visually.\n",
    "    \n",
    "    :returns: 2d matrix, {2d matrix index: state name}, {state name: 2d matrix}\n",
    "    \"\"\"\n",
    "    states = sorted(list(movements.keys()))\n",
    "    R = [[0 for _ in states] for _ in states]\n",
    "    for j, s_0 in enumerate(states):\n",
    "        for s_1, reward in zip(movements[s_0][\"moves\"], movements[s_0][\"reward\"]):\n",
    "            position = [i for i, s in enumerate(states) if s == s_1]\n",
    "            R[j][position[0]] = reward\n",
    "    return R, {ind: letter for ind, letter in enumerate(states)}, {letter: ind for ind, letter in enumerate(states)}\n",
    "\n",
    "\n",
    "def backwards_algo(options, r, t, state_num=NUM_STATES - 1):\n",
    "    \"\"\"\n",
    "    This is the main function, which implements the backward algorithm. It searches recursively for \n",
    "    the route that maximizes the score.\n",
    "    \n",
    "    INPUTS:\n",
    "    options: [(current score, current index, [current sequence]), ...] , all of the options with max scores \n",
    "    r: reward matrix\n",
    "    t: index of times ran\n",
    "    state_num = # of iterations to make, (equal to the number of states - 1, as the first iteration is made in the function call)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    :returns: [(final score, ending index, [state sequence])]\n",
    "    \"\"\"\n",
    "    if t < state_num:\n",
    "        option_container=[]\n",
    "        for j, (reward, move, state_history) in enumerate(options): # this loop handles multiple options with the same score\n",
    "            local_options = [(r[i][move] + reward, i, state_history + [i]) for i, row in enumerate(r) if row[move] > 0]\n",
    "            max_option = get_max_options(local_options)\n",
    "            option_container.extend(max_option)\n",
    "        return backwards_algo(options=get_max_options(option_container), r=r, t=t+1)\n",
    "    return options\n",
    "\n",
    "\n",
    "def get_max_options(reward_tuple):\n",
    "    \"\"\"\n",
    "    get_max_options is a helper function to find the max score from a list of options, \n",
    "    while preserving the other information in the tuple. it returns a list of all values == to the max reward, \n",
    "    as there are occurances where reward via 2 different routes are the same\n",
    "    \n",
    "    INPUTS:\n",
    "    reward_tuple: a list of reward tuples (same as options in bacckwards_algo)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    :returns: a list of tuples with the max reward\n",
    "    \"\"\"\n",
    "    max_reward = max(reward_tuple, key=lambda item:item[0])\n",
    "    return [(r, i, states) for r, i, states in reward_tuple if r == max_reward[0]]\n",
    "\n",
    "\n",
    "def compose_pretty_state_sequence(reverse_sequence, state_dict):\n",
    "    \"\"\"\n",
    "    compose_pretty_state_sequence is a helper function to reverse the sequence returned by the backwards algorithm. \n",
    "    It also converts the index sequnce into a state name sequnce\n",
    "    \n",
    "    INPUTS:\n",
    "    reverse_sequence: the final state sequence returned by the backwards algorithm\n",
    "    state_dict: a mapping of indexes to state names, created by compose_R\n",
    "    \n",
    "    OUTPUTS:\n",
    "    :return: the pretty state sequence\n",
    "    \"\"\"\n",
    "    return [state_dict[val] for val in reverse_sequence[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = \n",
      "[0, 1.0, 1.6, 1.9, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1.0, 1.5, 1.0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1.8, 1.8, 1.7, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1.9, 1.6, 1.5, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1.9, 1.6, 1.8, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1.4, 1.4, 1.8, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1.0, 1.1, 1.1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.8]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.8]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#  creating R\n",
    "R, *state_index = compose_R(MOVEMENTS)\n",
    "print(\"R = \")\n",
    "[print(r) for r in R];\n",
    "\n",
    "# getting the index of the final state (the starting state of the backwards algorithm)\n",
    "starting_state = [state_index[1]['P']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the route of max reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Result:\t(7.4, 0, [10, 9, 4, 3, 0])\n",
      "Score:\t\t7.4\n",
      "Sequence:\t['A', 'D', 'E', 'O', 'P']\n"
     ]
    }
   ],
   "source": [
    "final_data = backwards_algo(options=get_max_options([(R[i][starting_state[-1]], i, starting_state + [i]) for i, row in enumerate(R) if row[starting_state[-1]] > 0]), r=R, t=0)\n",
    "print(f\"Raw Result:\\t{final_data[0]}\")\n",
    "print(f\"Score:\\t\\t{final_data[0][0]}\\nSequence:\\t{compose_pretty_state_sequence(reverse_sequence=final_data[0][-1], state_dict=state_index[0])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
