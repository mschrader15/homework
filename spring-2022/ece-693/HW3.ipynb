{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mschrader15/homework/blob/main/spring-2022/ece-693/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "This homework explores image classification models described in the lecture/textbook. The exercise below repeats the major steps in the dog/cat classification but on a different dataset. \n",
        "\n",
        "The dataset used for this homework is described in the paper \"Food/Non-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model\", Ashutosh Singla, Lin Yuan, Touradj Ebrahimi, MADiMa '16: Proceedings of the 2nd International Workshop on Multimedia Assisted Dietary ManagementOctober 2016 Pages 3â€“11\n",
        "\n",
        "Link to the paper: https://dl.acm.org/doi/10.1145/2986035.2986039"
      ],
      "metadata": {
        "id": "bpUwBBWJRLqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**\n",
        "\n",
        "The official source of this dataset is hosted on EPFl's website but does not seem to be available. Thus we will use the version posted on Kaggle: https://www.kaggle.com/trolukovich/food5k-image-dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**You must use this specific Kaggle dataset (there are others) and preserve the directory structure of the dataset**\n",
        "\n",
        "\n",
        "---\n",
        "You will need to register on Kaggle as described in the textbook on pages 212-213 (or just follow Kaggle instructions).  \n",
        "\n",
        "The dataset contains 2500 food and 2500 non-food images, for the task of food/non-food classification. The whole dataset is divided in three parts: training, validation and evaluation. \n",
        "\n",
        "Each directory has food/non-food subdirectory containing images of the corresponding class. \n",
        "\n",
        "---\n",
        "In the following code cell define the path to the directory containing the dataset. When I'll be running the notebook, I'll modify the path to the directory on my drive.\n",
        "\n",
        "You also need to define a path to the secret dataset I'll use for additional testing of your models. This dataset will have the same structure:\n",
        "  - root directory\n",
        "    - food\n",
        "    - non-food. \n",
        "\n",
        "The data will be independent of Food-5K. Your code needs to compute the accuracy of food/non-food classification on this dataset. For the sake of development, create the directory structure and copy some images into those directories.\n"
      ],
      "metadata": {
        "id": "Cg5a4Dsxeqsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52jcctSlRGae"
      },
      "outputs": [],
      "source": [
        "#define the path to the food/non-food dataset\n",
        "import pathlib\n",
        "dataset_path=pathlib.Path('D:\\\\data\\\\Food-5K')\n",
        "secret_dataset_path=pathlib.Path('D:\\\\data\\\\Food-5K\\\\secret')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1**\n",
        "\n",
        "In this task you have to design and train a small CNN model following the procedure described in Chapter 8 of the book. \n",
        "\n",
        "Requirements:\n",
        "- use at least 256 x 256 image resolution\n",
        "- do not use exactly the same architecture as in the book, adapt the CNN to match increased image resolution\n",
        "- (recommendation) in preprocessing, consider preserving the aspect ratio of the original image while scaling\n",
        "- use data augmentation, add at least one adiditional augmentation method compared to the book (see https://www.tensorflow.org/tutorials/images/data_augmentation)\n",
        "- add RandomColorDistortion augmentation (see https://towardsdatascience.com/writing-a-custom-data-augmentation-layer-in-keras-2b53e048a98)\n",
        "- Train the model, display training and validation loss/accuracy curves\n",
        "- Evaluate the effect of adding dropout layers after convolutional layers. You may try various options (at least one dropout layer/no dropout), evaluate the effect of validation/test accuracy, document the experiment in the report and then keep the best model here.\n",
        "- **Save the model on the disk** This is your baseline model, everything else is compared to the results obatined by this model."
      ],
      "metadata": {
        "id": "PtYViHzbi2g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code goes here, do not forget to save the model"
      ],
      "metadata": {
        "id": "7_MKSXGgkJq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D_UbObtV6D7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2**\n",
        "\n",
        "- Load the model from the disk (required so that I can test without retraining)\n",
        "- Assess accuracy on the test set\n",
        "- Assess accuracy on the secret dataset (I'll use your code but my own dataset here, do not forget to use `secret_dataset_path`)\n",
        "- In the report comment on the architecture choices you made, effect of drop out layers.\n"
      ],
      "metadata": {
        "id": "Ehe8X9mmkFz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your solution goes here"
      ],
      "metadata": {
        "id": "r0_Fl88Vmui2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3**\n",
        "\n",
        "Here you need to use RESNET layers to perform the same classification task.\n",
        "\n",
        "Requirements:\n",
        "- double the number of convolution layers compared to the CNN model above (double the depth)\n",
        "- train for a number of epochs (sufficiently long to match or beat the baseline)\n",
        "- Replace the CNN layers by RESNET layers, train for the same number of epochs. Note the accuracy and training/validation curves. If needed train until matching/beating the baseline performance.\n",
        "- **Save the model on the disk**\n",
        "\n"
      ],
      "metadata": {
        "id": "ifXZv3PwmmyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4**\n",
        "\n",
        "- Load the model from the disk (required so that I can test without retraining)\n",
        "- Assess accuracy on the test set\n",
        "- Assess accuracy on the secret dataset (I'll use your code but my own dataset here, do not forget to use `secret_dataset_path`)\n",
        "- In the report comment the effects of RESNET layers on convergence speed, accuracy, etc."
      ],
      "metadata": {
        "id": "2hIxXvSs6er6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 5**\n",
        "\n",
        "Here you need to leverage CNN layers of a pre-trained model to perform the same classification task.\n",
        "\n",
        "Requirements:\n",
        "- use any pre-trained Keras model except VGG16, VGG19, RESNET-50 \n",
        "- as a source of the models you may use https://keras.io/api/applications/ or any other source on the web\n",
        "- train the classification layer using any of the two approaches described in the book, display training and validation curves\n",
        "- **Save the model on the disk**\n",
        "\n"
      ],
      "metadata": {
        "id": "bG0v4wLU5gF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your solution goes here, do not forget to save the model"
      ],
      "metadata": {
        "id": "zn1-Nr62oUpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 6**\n",
        "\n",
        "- Load the model from the disk (required so that I can test without retraining)\n",
        "- Assess accuracy on the test set\n",
        "- Assess accuracy on the secret dataset (I'll use your code but my own dataset here, do not forget to use `secret_dataset_path`)\n",
        "- In the report, comment on the effect of using the pretrained model on speed of convergence, accuracy, etc."
      ],
      "metadata": {
        "id": "o0rc3WySrhoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grading**\n",
        "\n",
        "- (10 pts) Report quality / submission requirement followed (see class policy)\n",
        "- (90 pts) Equally split between Task 1 - Task 6, accounting for correcteness of the approach and satifaction of the requirements above\n",
        "- (10 pts) Beat the paper accuracy on the secret dataset"
      ],
      "metadata": {
        "id": "Z7NOkqN_oYM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Report** (your report goes here, keep the length manageable)\n",
        "Report may also be submitted as a pdf document in the same compressed folder.\n",
        "\n",
        "- Introduction\n",
        "- Methods\n",
        "- Results\n",
        "- Discussion/Conclusions"
      ],
      "metadata": {
        "id": "B96jOT9Aoxfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submission**\n",
        "\n",
        "Using Blackboard, submit the .ipynb file and **saved models** in a compressed folder as defined by the class policy."
      ],
      "metadata": {
        "id": "d2B_yQvOpSqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hints**\n",
        "\n",
        "The code from chapter 8 is posted here: https://github.com/fchollet/deep-learning-with-python-notebooks\n",
        "\n",
        "The code needs minimal changes to run the very first approximation of the solution. Once that is operational, proceed to satisfy all assignment requirements."
      ],
      "metadata": {
        "id": "ka3_JZxVsSf0"
      }
    }
  ]
}